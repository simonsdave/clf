#!/usr/bin/env python
"""This module is the spider host's mainline."""

import logging


from clparser import CommandLineParser
import mainloop
from queues import CrawlRequestQueue
from queues import CrawlResponseQueue
from local_spider_repo import LocalSpiderRepo
from clf.util.rrsleeper import RRSleeper
from clf.util import tsh


_logger = logging.getLogger("CLF_%s" % __name__)


if __name__ == "__main__":
    tsh.install()

    clp = CommandLineParser()
    (clo, cla) = clp.parse_args()

    logging.basicConfig(level=clo.logging_level)

    fmt = (
        "Spider Host "
        "reading requests from queue '{clo.crawl_request_queue_name}', "
        "writing responses to queue '{clo.crawl_response_queue_name}', "
        "downloading spiders from spider repo '{clo.spider_repo_name}' "
        "and sleeping {clo.min_num_secs_to_sleep} to "
        "{clo.max_num_secs_to_sleep} seconds"
    )
    _logger.info(fmt.format(clo=clo))

    crawl_request_queue = CrawlRequestQueue.get_queue(
        clo.crawl_request_queue_name)
    if not crawl_request_queue:
        _logger.error(
            "Could not find request queue '%s'",
            clo.request_queue_name)
        sys.exit(1)

    crawl_response_queue = CrawlResponseQueue.get_queue(
        clo.crawl_response_queue_name)
    if not crawl_response_queue:
        _logger.error(
            "Could not find response queue '%s'",
            clo.response_queue_name)
        sys.exit(1)

    rr_sleeper = RRSleeper(
        clo.min_num_secs_to_sleep,
        clo.max_num_secs_to_sleep)

    local_spider_repo = LocalSpiderRepo(clo.spider_repo_name)
    if not local_spider_repo:
        _logger.error(
            "Could not establish local spider repo '%s'",
            local_spider_repo)
        sys.exit(1)

    mainloop.run(
        crawl_request_queue,
        crawl_response_queue,
        rr_sleeper,
        local_spider_repo)
